####################################
Tutorial 10: RAG ç”Ÿäº§éƒ¨ç½²
####################################

.. include:: ../links.ref
.. include:: ../tags.ref
.. include:: ../abbrs.ref

ç”Ÿäº§ç¯å¢ƒè€ƒé‡
============

å°† RAG ç³»ç»Ÿéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒéœ€è¦è€ƒè™‘ï¼š

- **æ€§èƒ½**: å“åº”æ—¶é—´ã€ååé‡
- **å¯é æ€§**: é«˜å¯ç”¨ã€å®¹é”™
- **å¯æ‰©å±•æ€§**: æ°´å¹³æ‰©å±•ã€è´Ÿè½½å‡è¡¡
- **å®‰å…¨æ€§**: æ•°æ®ä¿æŠ¤ã€è®¿é—®æ§åˆ¶
- **æˆæœ¬**: API è°ƒç”¨ã€å­˜å‚¨ã€è®¡ç®—

æ¶æ„è®¾è®¡
========

.. code-block:: text

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                    RAG ç”Ÿäº§æ¶æ„                                  â”‚
   â”‚                                                                  â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
   â”‚  â”‚                      è´Ÿè½½å‡è¡¡                              â”‚   â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
   â”‚                             â”‚                                    â”‚
   â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
   â”‚         â”‚                   â”‚                   â”‚               â”‚
   â”‚         â–¼                   â–¼                   â–¼               â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
   â”‚  â”‚  API Server â”‚     â”‚  API Server â”‚     â”‚  API Server â”‚          â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
   â”‚         â”‚                  â”‚                  â”‚                 â”‚
   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
   â”‚                            â”‚                                    â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
   â”‚  â”‚                         â–¼                         â”‚         â”‚
   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚         â”‚
   â”‚  â”‚  â”‚ å‘é‡æ•°æ®åº“ â”‚    â”‚  Redis   â”‚    â”‚   LLM    â”‚    â”‚         â”‚
   â”‚  â”‚  â”‚ (Milvus) â”‚    â”‚ (ç¼“å­˜)   â”‚    â”‚  (API)   â”‚    â”‚         â”‚
   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚         â”‚
   â”‚  â”‚                   æœåŠ¡å±‚                          â”‚         â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
   â”‚                                                                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FastAPI æœåŠ¡
============

.. code-block:: python

   # app/main.py
   from fastapi import FastAPI, HTTPException, BackgroundTasks
   from pydantic import BaseModel
   from typing import List, Optional
   import time

   app = FastAPI(title="RAG API", version="1.0.0")

   # è¯·æ±‚/å“åº”æ¨¡å‹
   class QueryRequest(BaseModel):
       question: str
       top_k: int = 3
       use_cache: bool = True

   class QueryResponse(BaseModel):
       answer: str
       sources: List[dict]
       latency_ms: float

   class IndexRequest(BaseModel):
       documents: List[str]
       metadatas: Optional[List[dict]] = None

   # RAG æœåŠ¡
   from app.rag_service import RAGService
   rag_service = RAGService()

   @app.post("/query", response_model=QueryResponse)
   async def query(request: QueryRequest):
       """æŸ¥è¯¢æ¥å£"""
       start_time = time.time()
       
       try:
           result = await rag_service.query(
               question=request.question,
               top_k=request.top_k,
               use_cache=request.use_cache
           )
           
           latency_ms = (time.time() - start_time) * 1000
           
           return QueryResponse(
               answer=result["answer"],
               sources=result["sources"],
               latency_ms=latency_ms
           )
       except Exception as e:
           raise HTTPException(status_code=500, detail=str(e))

   @app.post("/index")
   async def index_documents(
       request: IndexRequest,
       background_tasks: BackgroundTasks
   ):
       """ç´¢å¼•æ–‡æ¡£ï¼ˆåå°ä»»åŠ¡ï¼‰"""
       background_tasks.add_task(
           rag_service.index_documents,
           request.documents,
           request.metadatas
       )
       return {"message": "Indexing started", "count": len(request.documents)}

   @app.get("/health")
   async def health_check():
       """å¥åº·æ£€æŸ¥"""
       return {"status": "healthy"}

RAG æœåŠ¡å®ç°
============

.. code-block:: python

   # app/rag_service.py
   from langchain_community.vectorstores import Chroma
   from langchain_community.embeddings import HuggingFaceEmbeddings
   from langchain_openai import ChatOpenAI
   from typing import List, Dict, Optional
   import redis
   import hashlib
   import json

   class RAGService:
       """RAG æœåŠ¡"""
       
       def __init__(self):
           # åˆå§‹åŒ–ç»„ä»¶
           self.embeddings = HuggingFaceEmbeddings(
               model_name="sentence-transformers/all-MiniLM-L6-v2"
           )
           self.vectorstore = Chroma(
               persist_directory="./chroma_db",
               embedding_function=self.embeddings
           )
           self.llm = ChatOpenAI(
               model="gpt-3.5-turbo",
               temperature=0,
               request_timeout=30
           )
           
           # ç¼“å­˜
           self.cache = redis.Redis(host='localhost', port=6379, db=0)
           self.cache_ttl = 3600  # 1å°æ—¶
       
       async def query(
           self,
           question: str,
           top_k: int = 3,
           use_cache: bool = True
       ) -> Dict:
           """æ‰§è¡ŒæŸ¥è¯¢"""
           
           # æ£€æŸ¥ç¼“å­˜
           if use_cache:
               cached = self._get_from_cache(question)
               if cached:
                   return cached
           
           # æ£€ç´¢
           docs = self.vectorstore.similarity_search(question, k=top_k)
           
           # æ„å»ºä¸Šä¸‹æ–‡
           context = "\n\n".join([doc.page_content for doc in docs])
           
           # ç”Ÿæˆå›ç­”
           prompt = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

   ä¿¡æ¯ï¼š
   {context}

   é—®é¢˜ï¼š{question}

   å›ç­”ï¼š"""
           
           answer = self.llm.invoke(prompt).content
           
           result = {
               "answer": answer,
               "sources": [doc.metadata for doc in docs]
           }
           
           # å­˜å…¥ç¼“å­˜
           if use_cache:
               self._save_to_cache(question, result)
           
           return result
       
       def index_documents(
           self,
           documents: List[str],
           metadatas: Optional[List[dict]] = None
       ):
           """ç´¢å¼•æ–‡æ¡£"""
           self.vectorstore.add_texts(
               texts=documents,
               metadatas=metadatas
           )
       
       def _get_cache_key(self, question: str) -> str:
           return f"rag:{hashlib.md5(question.encode()).hexdigest()}"
       
       def _get_from_cache(self, question: str) -> Optional[Dict]:
           key = self._get_cache_key(question)
           cached = self.cache.get(key)
           if cached:
               return json.loads(cached)
           return None
       
       def _save_to_cache(self, question: str, result: Dict):
           key = self._get_cache_key(question)
           self.cache.setex(key, self.cache_ttl, json.dumps(result))

Docker éƒ¨ç½²
===========

.. code-block:: dockerfile

   # Dockerfile
   FROM python:3.11-slim

   WORKDIR /app

   # å®‰è£…ä¾èµ–
   COPY requirements.txt .
   RUN pip install --no-cache-dir -r requirements.txt

   # å¤åˆ¶ä»£ç 
   COPY app/ app/

   # ç¯å¢ƒå˜é‡
   ENV PYTHONPATH=/app
   ENV PYTHONUNBUFFERED=1

   EXPOSE 8000

   CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

.. code-block:: yaml

   # docker-compose.yml
   version: '3.8'

   services:
     api:
       build: .
       ports:
         - "8000:8000"
       environment:
         - OPENAI_API_KEY=${OPENAI_API_KEY}
         - REDIS_URL=redis://redis:6379
       depends_on:
         - redis
         - milvus
       volumes:
         - ./chroma_db:/app/chroma_db

     redis:
       image: redis:7-alpine
       ports:
         - "6379:6379"

     milvus:
       image: milvusdb/milvus:latest
       ports:
         - "19530:19530"
       volumes:
         - ./milvus_data:/var/lib/milvus

æ€§èƒ½ä¼˜åŒ–
========

1. æ‰¹é‡å¤„ç†
-----------

.. code-block:: python

   from typing import List
   import asyncio

   class BatchProcessor:
       """æ‰¹é‡å¤„ç†å™¨"""
       
       def __init__(self, rag_service, batch_size=10, max_wait_ms=100):
           self.rag_service = rag_service
           self.batch_size = batch_size
           self.max_wait_ms = max_wait_ms
           self.pending = []
           self.lock = asyncio.Lock()
       
       async def query(self, question: str) -> dict:
           """æ·»åŠ åˆ°æ‰¹é‡é˜Ÿåˆ—"""
           future = asyncio.Future()
           
           async with self.lock:
               self.pending.append((question, future))
               
               if len(self.pending) >= self.batch_size:
                   await self._process_batch()
           
           # ç­‰å¾…ç»“æœ
           return await future
       
       async def _process_batch(self):
           """å¤„ç†æ‰¹é‡è¯·æ±‚"""
           batch = self.pending[:self.batch_size]
           self.pending = self.pending[self.batch_size:]
           
           questions = [q for q, _ in batch]
           
           # æ‰¹é‡åµŒå…¥
           embeddings = self.rag_service.embeddings.embed_documents(questions)
           
           # æ‰¹é‡æ£€ç´¢å’Œç”Ÿæˆ
           for i, (question, future) in enumerate(batch):
               result = await self.rag_service.query(question)
               future.set_result(result)

2. å¼‚æ­¥å¤„ç†
-----------

.. code-block:: python

   import asyncio
   from concurrent.futures import ThreadPoolExecutor

   class AsyncRAGService:
       """å¼‚æ­¥ RAG æœåŠ¡"""
       
       def __init__(self):
           self.executor = ThreadPoolExecutor(max_workers=10)
           # ... åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
       
       async def query(self, question: str) -> dict:
           """å¼‚æ­¥æŸ¥è¯¢"""
           loop = asyncio.get_event_loop()
           
           # å¹¶è¡Œæ‰§è¡Œæ£€ç´¢å’Œå…¶ä»–æ“ä½œ
           retrieval_task = loop.run_in_executor(
               self.executor,
               self._retrieve,
               question
           )
           
           docs = await retrieval_task
           
           # ç”Ÿæˆå›ç­”
           answer = await self._generate_async(question, docs)
           
           return {"answer": answer, "sources": docs}

ç›‘æ§å’Œæ—¥å¿—
==========

.. code-block:: python

   import logging
   import time
   from prometheus_client import Counter, Histogram, generate_latest
   from functools import wraps

   # é…ç½®æ—¥å¿—
   logging.basicConfig(
       level=logging.INFO,
       format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
   )
   logger = logging.getLogger("rag")

   # Prometheus æŒ‡æ ‡
   REQUEST_COUNT = Counter(
       'rag_requests_total',
       'Total RAG requests',
       ['status']
   )

   REQUEST_LATENCY = Histogram(
       'rag_request_latency_seconds',
       'RAG request latency',
       buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
   )

   def monitor(func):
       """ç›‘æ§è£…é¥°å™¨"""
       @wraps(func)
       async def wrapper(*args, **kwargs):
           start_time = time.time()
           
           try:
               result = await func(*args, **kwargs)
               REQUEST_COUNT.labels(status='success').inc()
               return result
           except Exception as e:
               REQUEST_COUNT.labels(status='error').inc()
               logger.error(f"Error in {func.__name__}: {e}")
               raise
           finally:
               latency = time.time() - start_time
               REQUEST_LATENCY.observe(latency)
               logger.info(f"{func.__name__} completed in {latency:.3f}s")
       
       return wrapper

   # ä½¿ç”¨
   @monitor
   async def query(question: str):
       # ... æŸ¥è¯¢é€»è¾‘
       pass

   # æŒ‡æ ‡ç«¯ç‚¹
   @app.get("/metrics")
   async def metrics():
       from fastapi.responses import Response
       return Response(
           generate_latest(),
           media_type="text/plain"
       )

å®‰å…¨æœ€ä½³å®è·µ
============

.. code-block:: python

   from fastapi import Depends, HTTPException, Security
   from fastapi.security import APIKeyHeader
   from pydantic import BaseModel, validator
   import re

   # API Key è®¤è¯
   api_key_header = APIKeyHeader(name="X-API-Key")

   async def verify_api_key(api_key: str = Security(api_key_header)):
       if api_key != settings.api_key:
           raise HTTPException(status_code=403, detail="Invalid API key")
       return api_key

   # è¾“å…¥éªŒè¯
   class QueryRequest(BaseModel):
       question: str
       
       @validator('question')
       def validate_question(cls, v):
           if len(v) > 1000:
               raise ValueError('Question too long')
           if len(v) < 3:
               raise ValueError('Question too short')
           # é˜²æ­¢æ³¨å…¥
           if re.search(r'[<>{}]', v):
               raise ValueError('Invalid characters')
           return v

   # é™æµ
   from slowapi import Limiter
   from slowapi.util import get_remote_address

   limiter = Limiter(key_func=get_remote_address)

   @app.post("/query")
   @limiter.limit("10/minute")
   async def query(request: QueryRequest):
       # ...

å…³é”®æ¦‚å¿µæ€»ç»“
============

.. csv-table::
   :header: "æ–¹é¢", "å…³é”®ç‚¹"
   :widths: 25, 75

   "æ€§èƒ½", "ç¼“å­˜ã€æ‰¹å¤„ç†ã€å¼‚æ­¥"
   "å¯é æ€§", "é‡è¯•ã€é™çº§ã€å¥åº·æ£€æŸ¥"
   "å¯æ‰©å±•", "æ°´å¹³æ‰©å±•ã€è´Ÿè½½å‡è¡¡"
   "å®‰å…¨", "è®¤è¯ã€éªŒè¯ã€é™æµ"
   "ç›‘æ§", "æ—¥å¿—ã€æŒ‡æ ‡ã€å‘Šè­¦"

æ€»ç»“
====

æ­å–œä½ å®Œæˆäº† RAG å…¨éƒ¨æ•™ç¨‹ï¼

ä½ å·²ç»å­¦ä¹ äº†ï¼š

1. âœ… RAG åŸºæœ¬æ¦‚å¿µå’ŒåŸç†
2. âœ… æ–‡æ¡£åŠ è½½å’Œå¤„ç†
3. âœ… æ–‡æœ¬åˆ†å—ç­–ç•¥
4. âœ… å‘é‡åµŒå…¥æŠ€æœ¯
5. âœ… å‘é‡æ•°æ®åº“ä½¿ç”¨
6. âœ… æ£€ç´¢ç­–ç•¥ä¼˜åŒ–
7. âœ… Prompt å·¥ç¨‹
8. âœ… ç³»ç»Ÿè¯„ä¼°æ–¹æ³•
9. âœ… é«˜çº§ RAG æŠ€æœ¯
10. âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

ğŸ‰ ç¥ä½ åœ¨ RAG åº”ç”¨å¼€å‘ä¸­å–å¾—æˆåŠŸï¼
